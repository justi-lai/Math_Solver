{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mainly to just test RAG solutions for this type of problem. I will maybe try to implement a custom graphrag later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from neo4j import GraphDatabase\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"password\")\n",
    "model_id = \"gemma-2-9b-it\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\"\n",
    ")\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating vector database embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "df = pd.read_csv('mathinfo.csv')\n",
    "\n",
    "embeddings = embedding_model.encode(df['Problem'].tolist(), convert_to_tensor=True, device=device)\n",
    "df['embedding'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 384])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.tensor(df['embedding'].tolist(), dtype=torch.float32, device=device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the derivative of x^2?\n",
      "Top 5 most similar sentences:\n",
      "Score: 0.5572, Problem: Find the derivative of tan(x)\n",
      "Score: 0.5399, Problem: Find the derivative of arcsin(x)\n",
      "Score: 0.4920, Problem: Find the derivative of ln(sin(x))\n",
      "Score: 0.3991, Problem: Solve ∫ 1/(x^2 + 1) dx\n",
      "Score: 0.3675, Problem: Solve ∫ x ln(x) dx\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the derivative of x^2?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True, device=device)\n",
    "\n",
    "dot_scores = torch.nn.functional.cosine_similarity(query_embedding, embeddings)\n",
    "top_k = 5\n",
    "top_k_indices = torch.topk(dot_scores, top_k).indices\n",
    "dot_scores_list = dot_scores.tolist()\n",
    "print(f\"Top {top_k} most similar sentences:\")\n",
    "for i in top_k_indices:\n",
    "    print(f\"Score: {dot_scores_list[i]:.4f}, Problem: {df['Problem'][i.item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query_embedding, embeddings):\n",
    "    dot_scores = torch.nn.functional.cosine_similarity(query_embedding, embeddings)\n",
    "    return dot_scores, dot_scores.tolist()\n",
    "\n",
    "def retrieve_relevant_resources(query, df, embedding_model, top_k=5):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True, device=device)\n",
    "    problem = df['Problem'].tolist()\n",
    "    embeddings = embedding_model.encode(problem, convert_to_tensor=True, device=device)\n",
    "    dot_scores, dot_list = cosine_similarity(query_embedding, embeddings)\n",
    "    top_k_indices = torch.topk(dot_scores, top_k).indices\n",
    "    return top_k_indices, dot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sdpa attention\n"
     ]
    }
   ],
   "source": [
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "print(f\"Using {attn_implementation} attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated System Prompt:\n",
      "You are a helpful assistant that helps solve mathematical problems. Use the following examples to understand the format and type of responses expected:\n",
      "\n",
      "Example 1:\n",
      "Problem: A ladder is leaning against a wall. The ladder is 10 feet long, and the base is 6 feet away from the wall. How high up the wall does the ladder reach?\n",
      "Solution:\n",
      "Use the Pythagorean theorem: a^2 + b^2 = c^2\n",
      "Substitute values: 6^2 + b^2 = 10^2\n",
      "Solve for b: b = √(10^2 - 6^2) = 8\n",
      "Result: 8 feet\n",
      "\n",
      "Example 2:\n",
      "Problem: Maximize the area of a rectangle with a perimeter of 20 units\n",
      "Solution:\n",
      "Let length = x and width = y, and use the perimeter constraint: 2x + 2y = 20\n",
      "Express y in terms of x: y = 10 - x\n",
      "Area = x * y = x(10 - x) = 10x - x^2\n",
      "Find the derivative: A'(x) = 10 - 2x\n",
      "Set A'(x) = 0: 10 - 2x = 0, x = 5\n",
      "Substitute x = 5 into y = 10 - x: y = 5\n",
      "Result: Maximum area is 5 * 5 = 25\n",
      "\n",
      "\n",
      "You will be given a new problem. Use the format shown in the examples to provide a step-by-step solution.\n",
      "\n",
      "Solution:\n",
      "The maximum height is reached when the velocity of the ball is zero.  We can find the time it takes to reach the maximum height by taking the derivative of the height function and setting it equal to zero.\n",
      "\n",
      "1. **Find the velocity function:** \n",
      "   v(t) = h'(t) = 20 - 10t\n",
      "\n",
      "2. **Set the velocity equal to zero and solve for t:**\n",
      "   20 - 10t = 0\n",
      "   10t = 20\n",
      "   t = 2 seconds\n",
      "\n",
      "3. **Substitute the time back into the height function to find the maximum height:**\n",
      "   h(2) = (20 * 2) - (5 * 2^2) \n",
      "   h(2) = 40 - 20\n",
      "   h(2) = 20 meters\n",
      "\n",
      "\n",
      "Result: The maximum height reached by the ball is 20 meters.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"A ball is thrown vertically upward with an initial velocity of 20 m/s. The height of the ball after t seconds is given by the function:\n",
    "\n",
    "h(t) = 20t - 5t^2\n",
    "\n",
    "Question:\n",
    "What is the maximum height reached by the ball?\"\"\"\n",
    "\n",
    "def generate_system_prompt(examples):\n",
    "    system_prompt = \"You are a helpful assistant that helps solve mathematical problems. \" \\\n",
    "                    \"Use the following examples to understand the format and type of responses expected:\\n\\n\"\n",
    "    for i, example in enumerate(examples):\n",
    "        system_prompt += f\"Example {i + 1}:\\n\"\n",
    "        system_prompt += f\"Problem: {example['Problem']}\\n\"\n",
    "        system_prompt += \"Solution:\\n\"\n",
    "        for step in safe_parse_steps(example['Steps']):\n",
    "            system_prompt += f\"{step.strip()}\\n\"\n",
    "        system_prompt += \"\\n\"\n",
    "    system_prompt += \"\\nYou will be given a new problem. Use the format shown in the examples to provide a step-by-step solution.\\n\"\n",
    "    return system_prompt\n",
    "\n",
    "def safe_parse_steps(steps_str):\n",
    "    fixed_str = re.sub(r\"([A-Za-z])'(\\()\", r\"\\1\\\\'\\2\", steps_str)\n",
    "    try:\n",
    "        return ast.literal_eval(fixed_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing steps: {e}. Original string: {steps_str}\")\n",
    "        return []\n",
    "\n",
    "top_k = 2\n",
    "top_k_indices, dot_list = retrieve_relevant_resources(prompt, df, embedding_model, top_k=top_k)\n",
    "\n",
    "examples = [{'Problem': df['Problem'][i.item()], 'Steps': df['Steps'][i.item()]} for i in top_k_indices]\n",
    "system_prompt = generate_system_prompt(examples)\n",
    "print(\"Generated System Prompt:\")\n",
    "print(system_prompt)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=messages\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MathSolver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
