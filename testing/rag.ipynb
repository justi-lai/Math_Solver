{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mainly to just test RAG solutions for this type of problem. I will maybe try to implement a custom graphrag later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating vector database embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Load in sentences\n",
    "df = pd.read_csv('mathinfo.csv')\n",
    "\n",
    "# Sentences are encoded/embedded by calling model.encode()\n",
    "embeddings = embedding_model.encode(df['Description'].tolist(), convert_to_tensor=True, device=device)\n",
    "df['embedding'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 384])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.tensor(df['embedding'].tolist(), dtype=torch.float32, device=device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the derivative of x^2?\n",
      "Top 5 most similar sentences:\n",
      "Score: 0.6161, Operation: Derivative\n",
      "Score: 0.3148, Operation: Square Root\n",
      "Score: 0.2861, Operation: Integration\n",
      "Score: 0.1576, Operation: Exponentiation\n",
      "Score: 0.1270, Operation: Absolute Value\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the derivative of x^2?\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True, device=device)\n",
    "\n",
    "dot_scores = torch.nn.functional.cosine_similarity(query_embedding, embeddings)\n",
    "top_k = 5\n",
    "top_k_indices = torch.topk(dot_scores, top_k).indices\n",
    "dot_scores_list = dot_scores.tolist()\n",
    "print(f\"Top {top_k} most similar sentences:\")\n",
    "for i in top_k_indices:\n",
    "    print(f\"Score: {dot_scores_list[i]:.4f}, Operation: {df['Operation'][i.item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query_embedding, embeddings):\n",
    "    dot_scores = torch.nn.functional.cosine_similarity(query_embedding, embeddings)\n",
    "    return dot_scores, dot_scores.tolist()\n",
    "\n",
    "def retrieve_relevant_resources(query, df, embedding_model, top_k=5):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True, device=device)\n",
    "    names = df['Operation'].tolist()\n",
    "    descriptions = df['Description'].tolist()\n",
    "    text = [f\"{name}: {description}\" for name, description in zip(names, descriptions)]\n",
    "    embeddings = embedding_model.encode(text, convert_to_tensor=True, device=device)\n",
    "    dot_scores, dot_list = cosine_similarity(query_embedding, embeddings)\n",
    "    top_k_indices = torch.topk(dot_scores, top_k).indices\n",
    "    return top_k_indices, dot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sdpa attention\n"
     ]
    }
   ],
   "source": [
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "print(f\"Using {attn_implementation} attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gemma-2-9b-it\n"
     ]
    }
   ],
   "source": [
    "model_id = \"gemma-2-9b-it\"\n",
    "print(f\"Using model: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great question!  \n",
      "\n",
      "The derivative of x² is **2x**. \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **The Power Rule:** We use a rule called the power rule for derivatives. The power rule states that the derivative of x<sup>n</sup> is nx<sup>n-1</sup>.\n",
      "\n",
      "* **Applying the Rule:** In this case, n = 2. So, applying the power rule:\n",
      "   *  d/dx (x²) = 2x<sup>(2-1)</sup> = 2x¹ = 2x\n",
      "\n",
      "\n",
      "Let me know if you'd like to see more examples or have any other calculus questions!\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a math tutor helping a student with calculus homework\"\n",
    "prompt = \"What is the derivative of x^2?\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}, \n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=messages\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the operation names in order of relevance:\n",
      "\n",
      "1. **Differentiation** \n",
      "2. **Substitution**  \n",
      "\n",
      "Top 5 most relevant operations:\n",
      "Score: 0.4660, Operation: Derivative\n",
      "Score: 0.3025, Operation: Integration\n",
      "Score: 0.2377, Operation: Exponentiation\n",
      "Score: 0.1922, Operation: Multiplication\n",
      "Score: 0.1529, Operation: Division\n",
      "You are a math tutor helping a student with math homework. The following are related operations to the question in order from most relevant to least relevant: \n",
      "Derivative\n",
      "Integration\n",
      "Exponentiation\n",
      "Multiplication\n",
      "Division\n",
      "\n",
      "Please break the question down step-by-step for the student. Here is the student's question: \n",
      "Here's how to break down this problem step-by-step:\n",
      "\n",
      "**1. Understand the Problem:**\n",
      "\n",
      "* We have a function, V(t), that tells us the volume of water in the tank at any given time (t).  \n",
      "* The question asks for the *rate* of change of the volume – essentially how fast the volume is increasing or decreasing at a specific moment (3 minutes in this case).\n",
      "\n",
      "**2. Recognize the Key Concept:**\n",
      "\n",
      "*  The rate of change of a function is found using its **derivative**. \n",
      "\n",
      "**3. Find the Derivative:**\n",
      "\n",
      "* The power rule of differentiation states that the derivative of  x<sup>n</sup> is nx<sup>n-1</sup>.\n",
      "* Applying this to our function V(t) = 4t² + 10, we get:\n",
      "    * V'(t) = 8t   (The derivative of 4t² is 8t, and the derivative of 10 (a constant) is 0).\n",
      "\n",
      "**4. Evaluate the Derivative at t = 3:**\n",
      "\n",
      "*  V'(3) = 8 * 3 = 24\n",
      "\n",
      "**5. Interpret the Result:**\n",
      "\n",
      "* V'(3) = 24 means that after 3 minutes, the volume of water in the tank is changing at a rate of **24 cubic units per minute**.\n",
      "\n",
      "\n",
      "Let me know if you'd like to work through another example or have any more questions!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Water is being poured into a cylindrical tank at a constant rate. The volume of water in the tank after t minutes is given by the function:\n",
    "\n",
    "V(t) = 4t^2 + 10\n",
    "\n",
    "Question:\n",
    "At what rate is the volume of water changing after 3 minutes?\"\"\"\n",
    "system_preprompt = \"\"\"You are a machine learning model that summarizes math problems for students. \n",
    "The following is a math problem. Return only the operation NAMES necessary to solve the problem in order from most relevant to least relevant, but DO NOT solve the problem: \"\"\"\n",
    "messages = [{\"role\": \"system\", \"content\": system_preprompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=messages\n",
    ")\n",
    "preprompt = response.choices[0].message.content\n",
    "print(preprompt)\n",
    "\n",
    "system_prompt_1 = \"You are a math tutor helping a student with math homework. The following are related operations to the question in order from most relevant to least relevant: \"\n",
    "system_prompt_2 = \"Please break the question down step-by-step for the student. Here is the student's question: \"\n",
    "# system_prompt_1 = \"You are a math tutor helping a student with math homework. The following are the related operations to the question and their sympy representations in order from most relevant to least relevant: \"\n",
    "# system_prompt_2 = \"Please break the question down step-by-step for the student and give the sympy representation of the question at each step. Here is the student's question: \"\n",
    "\n",
    "top_k = 5\n",
    "top_k_indices, dot_list = retrieve_relevant_resources(preprompt, df, embedding_model, top_k=top_k)\n",
    "print(f\"Top {top_k} most relevant operations:\")\n",
    "for i in top_k_indices:\n",
    "    print(f\"Score: {dot_list[i]:.4f}, Operation: {df['Operation'][i.item()]}\")\n",
    "\n",
    "operations_prompt = \"\"\n",
    "for i in top_k_indices:\n",
    "    operations_prompt += f\"{df['Operation'][i.item()]}\\n\"\n",
    "    # operations_prompt += f\"{df['Operation'][i.item()]} | {df['Sympy Representation'][i.item()]}\\n\"\n",
    "\n",
    "appended_system_prompt = f\"{system_prompt_1}\\n{operations_prompt}\\n{system_prompt_2}\"\n",
    "print(appended_system_prompt)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": appended_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "response = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=messages\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MathSolver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
